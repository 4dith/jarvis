# JARVIS Web Search & Summarization

This project is a **JARVIS AI feature** that allows users to ask questions via **text or voice**, performs **web search**, summarizes the answers in a **ChatGPT-style format**, and provides the **source links**. Optional **voice output** can also read the summarized answer aloud.  

It is built entirely using **open-source tools** and **free technologies**, suitable for quick prototyping.

---

## üöÄ Features

- **Text & Voice Input**: Ask queries via typing or speaking (optional).
- **Fast Web Search**: Uses DuckDuckGo, Google, and Yahoo search engines.
- **Content Fetching**: Retrieves page content using concurrent requests for speed.
- **Summarization**: AI-powered summarization of web content for concise answers.
- **Source Links**: Displays clickable links for all referenced sources.
- **Voice Output**: Converts the answer into speech (optional).

---

## üõ† Tech Stack

- **Backend**: Python, Flask
- **Web Scraping**: Requests, BeautifulSoup4, Newspaper3k
- **Search API**: DDGS (DuckDuckGo Search), Optional: Google Search API
- **NLP & Summarization**: Hugging Face Transformers, Sentence-Transformers
- **Concurrency**: `concurrent.futures.ThreadPoolExecutor`
- **Frontend**: HTML, CSS (Responsive), Optional Voice Buttons

---

## ‚ö° Optimizations Implemented

1. **Concurrent Page Fetching** ‚Üí Faster retrieval of web content.
2. **Token-based Summarization** ‚Üí Accurate chunk-based summaries.
3. **Batch Processing** ‚Üí Faster inference on GPU/CPU.
4. **Caching** ‚Üí Summarizer, tokenizer, and page content cached.
5. **Fallback Mechanisms** ‚Üí Avoids crashes when summarization or fetching fails.
6. **Mixed Precision** ‚Üí Efficient GPU utilization (`fp16`) for summarization.
7. **Minimal Search Results** ‚Üí Limits top 3‚Äì5 results for speed.

---

## üìÅ Project Structure

```
web_search/
‚îú‚îÄ app.py # Flask application entry point
‚îú‚îÄ requirements.txt # Python dependencies
‚îú‚îÄ templates/
‚îÇ ‚îî‚îÄ index.html # Frontend HTML template
‚îú‚îÄ core/
‚îÇ ‚îú‚îÄ handler.py # Orchestrates search, fetch, rerank, summarize
‚îÇ ‚îú‚îÄ search.py # Web search implementation
‚îÇ ‚îú‚îÄ fetcher.py # Page fetching and parsing
‚îÇ ‚îú‚îÄ rerank.py # Snippet reranking (optional)
‚îÇ ‚îî‚îÄ summarize.py # Optimized text summarization
‚îî‚îÄ config.py # Configuration (search limits, summarizer model, etc.)
```
----

## üì¶ Installation

1. Clone the repository:
```bash
git clone https://github.com/your-username/jarvis-web-search.git
python3 -m venv venv   #activate environment
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows
cd jarvis-web-search
pip install -r requirements.txt
pip install accelerate sentencepiece  ## optional dependencies for faster summarization
python app.py
```
---

## üîß Configuration
```
NUM_SEARCH_RESULTS = 5        # Limit number of search results
MAX_PAGES = 3                 # Max pages to fetch
MAX_CHARS = 600               # Max characters per summary
MAX_WORKERS = 5               # Thread pool size for fetching
SUMMARIZER_MODEL = "sshleifer/distilbart-cnn-12-6"
```
**Remarks** - Adjust these values to balance speed, accuracy, and performance.

---

## Optimiztions

‚öôÔ∏è Files to Modify for Speed Improvements
1. üß© core/handler.py

- This is the main target ‚Äî it handles web search and summarization.
- **Optimizations to apply here:**

    - Use async I/O (aiohttp + asyncio) for concurrent fetching of web pages.

    - Cache results using functools.lru_cache or a lightweight cache (like SQLite or diskcache).

    - Use a lightweight summarizer (e.g., t5-small, bart-mini, or Gemma-2b-it via Ollama).

    - Limit the number of web pages fetched (e.g., top 3 search results).

2. üß† core/search.py

- Handles fetching search results (Google, DuckDuckGo, or Serper API).

**Optimizations:**

- Use DuckDuckGo‚Äôs HTML API instead of Google scraping ‚Üí much faster and rate-limit friendly.

- Fetch only the top 3‚Äì5 links.

- Cache results locally (so repeated queries don‚Äôt hit the network).

3. üìù core/summarizer.py

- Handles text summarization.

**Optimizations:**

- Replace large models (like t5-large or bart-large) with smaller ones (t5-small, distilbart-cnn, etc.).

- Limit text input length before summarization (e.g., 1024 tokens).

- Optionally, use extractive summarization (sumy, spacy, or transformers.pipeline("summarization")) instead of generative if time is critical.

## handler.py

| Optimization               | Description                                    | Speed Gain                      |
| -------------------------- | ---------------------------------------------- | ------------------------------- |
| **Caching (LRU)**          | Avoids redundant network + summarization calls | ~2√ó faster for repeated queries |
| **Concurrent Fetching**    | Fetches all top links simultaneously           | ~3√ó faster web page retrieval   |
| **Text Truncation**        | Summarizes only key parts                      | Less model inference time       |
| **Async-ready**            | Designed for future async migration            | Lower blocking time             |
| **Smarter error handling** | Keeps system stable under network errors       | More reliable runtime           |

## search.py

| Optimization                 | Purpose                              | Speed Gain                     |
| ---------------------------- | ------------------------------------ | ------------------------------ |
| **LRU Cache**                | Stores previous searches (in memory) | 2√ó faster for repeated queries |
| **Retry + Backoff**          | Handles network/API instability      | Higher reliability             |
| **Parallel Fallback (Lite)** | Uses multiple small DDG queries      | More consistent coverage       |
| **Deduplication**            | Removes redundant results            | Cleaner summaries              |
| **Logging + Timing**         | Tracks performance per query         | Debug friendly                 |

## summarizer.py

| Optimization                | Description                                   | Gain                    |
| --------------------------- | --------------------------------------------- | ----------------------- |
| **Token-based chunking**    | Prevents mid-sentence cuts, preserves context | +15% accuracy           |
| **Batch inference**         | Summarizes multiple chunks at once            | 2√ó‚Äì4√ó faster            |
| **Auto device mapping**     | Uses GPU if available                         | 3√ó‚Äì10√ó faster           |
| **Adaptive min/max length** | Adjusts summary size to input length          | Better balance          |
| **Caching & lazy loading**  | Load model only once                          | Fast subsequent queries |
| **FP16 precision**          | Reduces compute overhead on GPU               | +30% speedup            |

## fetcher.py

| Optimization               | Description                                              | Effect                      |
| -------------------------- | -------------------------------------------------------- | --------------------------- |
| **httpx client**           | Replaces `requests`, supports async + faster connections | 1.5‚Äì2√ó faster               |
| **Selectolax**             | Much faster HTML parsing than BeautifulSoup              | 5√ó speedup                  |
| **Local caching**          | Saves cleaned text to disk, avoids refetching            | Near-instant repeat queries |
| **Smaller retry count**    | Prevents hanging on bad sites                            | More responsive             |
| **Token limit truncation** | Prevents processing huge pages                           | Memory safe                 |
| **Thread-safe caching**    | Works with `ThreadPoolExecutor` in `handler.py`          | Stable under concurrency    |

## üìà Future Improvements

- Integrate more search engines for broader coverage.
- Add multi-language support.
- Improve voice input & output with offline models.
- Enhance snippet reranking with embeddings for better relevance.

